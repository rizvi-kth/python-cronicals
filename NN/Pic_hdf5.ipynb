{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_data = True  # shuffle the addresses before saving\n",
    "hdf5_path = 'train/dataset.hdf5'  # address to where you want to save the hdf5 file\n",
    "cat_dog_train_path = 'train/*.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read addresses and labels from the 'train' folder\n",
    "addrs = glob.glob(cat_dog_train_path)\n",
    "labels = [1 if 'cat' in addr else 0 for addr in addrs]  # 0 = Cat, 1 = Dog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train\\\\cat.0.jpg', 'train\\\\cat.1.jpg', 'train\\\\cat.10.jpg', 'train\\\\cat.2.jpg', 'train\\\\cat.3.jpg', 'train\\\\cat.4.jpg', 'train\\\\cat.5.jpg', 'train\\\\cat.6.jpg', 'train\\\\cat.7.jpg', 'train\\\\cat.8.jpg', 'train\\\\cat.9.jpg', 'train\\\\dog.0.jpg', 'train\\\\dog.1.jpg', 'train\\\\dog.10.jpg', 'train\\\\dog.2.jpg', 'train\\\\dog.3.jpg', 'train\\\\dog.4.jpg', 'train\\\\dog.5.jpg', 'train\\\\dog.6.jpg', 'train\\\\dog.7.jpg', 'train\\\\dog.8.jpg', 'train\\\\dog.9.jpg']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Divide the hata into 60% train, 20% validation, and 20% test\n",
    "#train_addrs = addrs[0:int(0.6*len(addrs))]\n",
    "#train_labels = labels[0:int(0.6*len(labels))]\n",
    "\n",
    "# Divide the data into 100% train\n",
    "train_addrs = addrs[0:int(len(addrs))]\n",
    "train_labels = labels[0:int(len(labels))]\n",
    "\n",
    "print(train_addrs)\n",
    "print(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_shape = (len(train_addrs), 224, 224, 3)\n",
    "print(train_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"dataset.hdf5\" (mode r+)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# open a hdf5 file and create arrays\n",
    "hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "print(hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"train_img\": shape (22, 224, 224, 3), type \"|i1\">"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf5_file.create_dataset(\"train_img\", train_shape, np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.create_dataset(\"train_labels\", (len(train_addrs),), np.int8)\n",
    "hdf5_file[\"train_labels\"][...] = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it's time to read images one by one, apply preprocessing (only resize in our code) and then save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# loop over train addresses\n",
    "for i in range(len(train_addrs)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if i % 1000 == 0 and i > 1:\n",
    "        print ('Train data: {}/{}'.format(i, len(train_addrs)))\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    addr = train_addrs[i]\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # add any image pre-processing here\n",
    "    # if the data order is Theano, axis orders should change\n",
    "    #if data_order == 'th':\n",
    "    #    img = np.rollaxis(img, 2)\n",
    "    # save the image and calculate the mean so far\n",
    "    hdf5_file[\"train_img\"][i, ...] = img[None]\n",
    "    #mean += img / float(len(train_labels))\n",
    "\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the HDF5 for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Samples  22\n",
      " Samples shape  (22, 224, 224, 3)\n",
      " Lables shape  (22,)\n",
      "Lables:\n",
      "<HDF5 dataset \"train_labels\": shape (22,), type \"|i1\">\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "hdf5_path = 'train/dataset.hdf5'\n",
    "subtract_mean = False\n",
    "# open the hdf5 file\n",
    "hdf5_file = h5py.File(hdf5_path, \"r\")\n",
    "# subtract the training mean\n",
    "if subtract_mean:\n",
    "    mm = hdf5_file[\"train_mean\"][0, ...]\n",
    "    mm = mm[np.newaxis, ...]\n",
    "# Total number of samples\n",
    "data_num = hdf5_file[\"train_img\"].shape[0]\n",
    "data_shape = hdf5_file[\"train_img\"].shape\n",
    "print(' Number of Samples ',data_num)\n",
    "print(' Samples shape ',data_shape)\n",
    "\n",
    "label_num = hdf5_file[\"train_labels\"].shape\n",
    "\n",
    "print(' Lables shape ',label_num)\n",
    "print('Lables:')\n",
    "print(hdf5_file[\"train_labels\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
