To start the Hadoop cluster for spark
=====================================
cd C:\tools\hadoop-2.8.3\sbin
start-all.cmd

# Check the Java processes are running
jps

# Start Spark with python 
cd C:\tools\spark2\bin
pyspark


To run Spark-driver with anaconda python change system wide python
====================================================================
From 1: C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64
to   2: C:\ProgramData\Anaconda3

Change it back to 1: to work with pip and Microsoft environment
===============================================================

Environment Variables for Spark
===============================
PYSPARK_SUBMIT_ARGS="pyspark-shell"
PYSPARK_DRIVER_PYTHON=jupyter
PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark

#PYSPARK_PYTHON=


Problem 1: Exception
================
Python in worker has different version 3.6 than that in driver 3.5, PySpark cannot run with different minor versions.
Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.

FIX_1:
python --version
conda search python
conda install python=3.6.3
