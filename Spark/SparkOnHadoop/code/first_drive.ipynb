{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://AF-35G7SF2.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make any python list to RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:184"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_RDD = sc.parallelize([1, \"Rizvi\", 20.5])\n",
    "simple_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n",
      "[1, 'Rizvi']\n",
      "[1, 'Rizvi', 20.5]\n"
     ]
    }
   ],
   "source": [
    "print(simple_RDD.count())\n",
    "print(simple_RDD.first())\n",
    "print(simple_RDD.take(2))\n",
    "print(simple_RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make any RDD to DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can NOT convert simple_RDD to DataFrame as this is a one-dymentional array and has no associated schema.\n",
    "simple_RDD.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Riz', 5.6], [2, 'Rid', 10.6]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mareix_rdd = sc.parallelize([[1,\"Riz\", 5.6],[2,\"Rid\", 10.6]])\n",
    "mareix_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string, _3: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mareix_df = mareix_rdd.toDF()\n",
    "mareix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing Row object for DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Riz', Nr=1, Score=5.6), Row(Name='Rid', Nr=2, Score=10.6)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mareix_rdd = sc.parallelize([Row(Nr=1,Name=\"Riz\",Score=5.6),\n",
    "                             Row(Nr=2,Name=\"Rid\",Score=10.6)])\n",
    "mareix_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mareix_df = mareix_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n",
      "|Name| Nr|Score|\n",
      "+----+---+-----+\n",
      "| Riz|  1|  5.6|\n",
      "| Rid|  2| 10.6|\n",
      "+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mareix_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame with complex data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_rdd = sc.parallelize([Row(col_list=[1,4,8],\n",
    "                                  col_dict={\"Riz\":300, \"Rid\":600},\n",
    "                                  col_row=Row(a=5.6, b=4.7, c=\"beta\"),\n",
    "                                  col_time=datetime(2015,12,31,12,59,59)\n",
    "                                 ),\n",
    "                             Row(col_list=[1,4,8],\n",
    "                                  col_dict={\"Riz\":300, \"Rid\":700},\n",
    "                                  col_row=Row(a=5.6, b=4.7, c=\"alpha\"),\n",
    "                                  col_time=datetime(2017,12,31,12,59,59))\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+-------------------+\n",
      "|            col_dict| col_list|          col_row|           col_time|\n",
      "+--------------------+---------+-----------------+-------------------+\n",
      "|[Rid -> 600, Riz ...|[1, 4, 8]| [5.6, 4.7, beta]|2015-12-31 12:59:59|\n",
      "|[Rid -> 700, Riz ...|[1, 4, 8]|[5.6, 4.7, alpha]|2017-12-31 12:59:59|\n",
      "+--------------------+---------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_df = complex_rdd.toDF()\n",
    "complex_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cx = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_cx.range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe with toupls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|Riz|200|\n",
      "|Rid|500|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql_cx.createDataFrame([(\"Riz\",200),(\"Rid\", 500)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Names|Scores|\n",
      "+-----+------+\n",
      "|  Riz|   200|\n",
      "|  Rid|   500|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql_cx.createDataFrame([(\"Riz\",200),(\"Rid\", 500)], [\"Names\", \"Scores\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing entire RDD\n",
    "###### If attempting to print out the elements of an RDD using rdd.foreach(println) or rdd.map(println) \n",
    "- On a single machine, this will generate the expected output and print all the RDD’s elements. \n",
    "- However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! \n",
    "- To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). This can cause the driver to run out of memory, though, because collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd = sc.parallelize([Row(1,\"Riz\"),\n",
    "                           Row(2,\"Rid\")])\n",
    "data_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-0fdc8e2d96e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\tools\\spark2\\python\\pyspark\\sql\\types.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1579\u001b[0m                                          for k, v in zip(self.__fields__, tuple(self)))\n\u001b[0;32m   1580\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1581\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"<Row(%s)>\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "# This print will fail : Row objects dont have any column defines\n",
    "print(data_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define colulmn with lambda and map() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[107] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = Row(\"ID\", \"NAME\")\n",
    "posts = data_rdd.map(lambda r: column_names(*r))\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(ID=1, NAME='Riz'), Row(ID=2, NAME='Rid')]\n"
     ]
    }
   ],
   "source": [
    "print(posts.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|NAME|\n",
      "+---+----+\n",
      "|  1| Riz|\n",
      "|  2| Rid|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stark_df = sql_cx.createDataFrame(posts)\n",
    "stark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interchangable between Spark DataFrame and Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Riz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Rid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID NAME\n",
       "0   1  Riz\n",
       "1   2  Rid"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = stark_df.toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| ID|NAME|\n",
      "+---+----+\n",
      "|  1| Riz|\n",
      "|  2| Rid|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_cx.createDataFrame(pandas_df).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
