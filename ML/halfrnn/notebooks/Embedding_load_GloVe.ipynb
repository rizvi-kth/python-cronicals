{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on \n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pip install -q -U tensorflow==1.7.0\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 1,\n",
       " 'done': 2,\n",
       " 'good': 3,\n",
       " 'effort': 4,\n",
       " 'poor': 5,\n",
       " 'well': 6,\n",
       " 'great': 7,\n",
       " 'nice': 8,\n",
       " 'excellent': 9,\n",
       " 'weak': 10,\n",
       " 'not': 11,\n",
       " 'could': 12,\n",
       " 'have': 13,\n",
       " 'better': 14}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Sequence from Text (Non-prefared way)\n",
    "Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 14], [6, 7], [11, 10], [14, 7], [9], [14], [7, 10], [8, 6], [7, 7], [5, 5, 14, 4]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "# vocab_size\n",
    "encoded_docs = [keras.preprocessing.text.one_hot(text, vocab_size) for text in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4. Again, we can do this with a built in Keras function, in this case the pad_sequences() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 14  0  0]\n",
      " [ 6  7  0  0]\n",
      " [11 10  0  0]\n",
      " [14  7  0  0]\n",
      " [ 9  0  0  0]\n",
      " [14  0  0  0]\n",
      " [ 7 10  0  0]\n",
      " [ 8  6  0  0]\n",
      " [ 7  7  0  0]\n",
      " [ 5  5 14  4]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Sequence from Text (Non-prefared way)\n",
    "tokenizer.texts_to_matrix(texts) function\n",
    "    - Return: numpy array of shape (len(texts), num_words).\n",
    "    - Arguments:\n",
    "        - texts: list of texts to vectorize.\n",
    "        - mode: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(t.texts_to_matrix(docs, mode='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Sequence from Text (prefared way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sequences :\n",
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "Text sequences padded :\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "text_seq = t.texts_to_sequences(docs)\n",
    "print('Text sequences :')\n",
    "print( text_seq)\n",
    "text_seq_padded = keras.preprocessing.sequence.pad_sequences(text_seq, maxlen=max_length, padding='post')\n",
    "print('Text sequences padded :')\n",
    "print(text_seq_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the entire GloVe word embedding file into memory as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../data/raw/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index.get('the').shape)\n",
    "print(embeddings_index.get('the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "word_feature = 100\n",
    "embedding_matrix = np.zeros((vocab_size, word_feature))\n",
    "for word, index in t.word_index.items():    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16190001e-01,  4.54470009e-01, -6.92160010e-01,\n",
       "         3.45799997e-02,  2.63480008e-01, -3.81390005e-01,\n",
       "        -2.27899998e-01,  3.72330010e-01, -2.05789998e-01,\n",
       "         2.90199995e-01,  1.21140003e-01, -4.27289993e-01,\n",
       "         5.55729985e-01, -9.42860022e-02, -4.99669999e-01,\n",
       "        -2.94779986e-01,  7.41090000e-01,  2.51910001e-01,\n",
       "        -2.74679989e-01,  2.31910005e-01,  3.82039999e-03,\n",
       "         4.52519991e-02,  2.49699995e-01, -4.15789992e-01,\n",
       "         3.13069999e-01, -5.84959984e-01, -3.27389985e-01,\n",
       "        -6.61889970e-01,  1.49090007e-01, -2.57710010e-01,\n",
       "        -9.48580027e-01,  4.18089986e-01, -2.95379996e-01,\n",
       "        -4.27110009e-02, -6.99699998e-01,  5.78920007e-01,\n",
       "        -6.92709982e-02, -3.96329984e-02, -5.64630004e-03,\n",
       "        -2.96160012e-01, -5.74479997e-01,  1.60099998e-01,\n",
       "        -1.06710002e-01,  1.00960001e-01, -4.29560006e-01,\n",
       "        -2.77850002e-01, -3.00170004e-01, -6.95370018e-01,\n",
       "         1.79649994e-01, -4.67229992e-01,  1.25110000e-01,\n",
       "        -2.90220007e-02, -1.59740001e-01,  1.42170000e+00,\n",
       "        -2.62239993e-01, -2.37190008e+00, -1.19170003e-01,\n",
       "        -5.82620025e-01,  1.55480003e+00,  4.22120005e-01,\n",
       "         8.46330002e-02,  1.13849998e+00, -3.12260002e-01,\n",
       "        -5.07379994e-02,  1.09360003e+00, -1.43700000e-03,\n",
       "         4.46410000e-01,  3.46249998e-01,  4.39639986e-01,\n",
       "        -3.89409989e-01,  2.70819992e-01,  8.06260034e-02,\n",
       "        -5.64810000e-02, -4.10970002e-01,  5.64199984e-01,\n",
       "        -1.51580006e-01, -1.49309993e-01,  1.71990007e-01,\n",
       "        -6.14950001e-01, -1.58219993e-01,  3.28179985e-01,\n",
       "         4.97559994e-01, -3.29470009e-01,  1.62220001e-01,\n",
       "        -2.30629992e+00,  4.06809986e-01,  2.77020007e-01,\n",
       "        -3.70020002e-01, -8.09689999e-01, -7.62560010e-01,\n",
       "         9.11090001e-02, -6.14730000e-01,  2.52860010e-01,\n",
       "         5.99800013e-02, -2.19769999e-01,  7.23819993e-03,\n",
       "        -3.38770002e-01, -5.47370017e-01,  4.88220006e-01,\n",
       "         3.22459996e-01]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer with GloVe word embedding weights\n",
    "The embedding layer can be seeded with the GloVe word embedding weights. We chose the 50-dimensional (or 100-dimensional) version, therefore the Embedding layer must be defined with output_dim set to **50** (or **100**). Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, word_feature, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 4, 100)            1500      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 1,901\n",
      "Trainable params: 401\n",
      "Non-trainable params: 1,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
