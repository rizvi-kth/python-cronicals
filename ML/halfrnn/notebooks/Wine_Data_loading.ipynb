{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is based on \n",
    " - https://medium.com/tensorflow/predicting-the-price-of-wine-with-the-keras-functional-api-and-tensorflow-a95d1c2c1b03\n",
    " - https://www.kaggle.com/learn/embeddings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pip install -q -U tensorflow==1.7.0\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "path_data = \"../data/raw/winemag-data_first150k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  ['Unnamed: 0', 'country', 'description', 'designation', 'points', 'price', 'province', 'region_1', 'region_2', 'variety', 'winery']\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "raw_data_df = pd.read_csv(path_data)\n",
    "cols = [c for c in raw_data_df.columns]\n",
    "print(\"Columns: \", cols)\n",
    "# raw_data_df['description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing descriptions as bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature 1:\n",
    "f_description = raw_data_df['description']\n",
    "vocab_size = 12000 # This is a hyperparameter, experiment with different values for your dataset\n",
    "# Create a tokenizer to preprocess our text descriptions\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size, char_level=False)\n",
    "tokenize.fit_on_texts(f_description )  # only fit on train\n",
    "# Sparse bag of words (bow) vocab_size vector\n",
    "description_bag_train = tokenize.texts_to_matrix(f_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_bag_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This tremendous 100% varietal wine hails from ...\n",
       "1    Ripe aromas of fig, blackberry and cassis are ...\n",
       "2    Mac Watson honors the memory of a wine once ma...\n",
       "3    This spent 20 months in 30% new French oak, an...\n",
       "4    This is the top wine from La Bégude, named aft...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing descriptions as a word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation**: We’ll first need to convert each description to a vector of integers corresponding to each word in our vocabulary. We can do that with the handy Keras texts_to_sequences method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7,\n",
       "  1695,\n",
       "  397,\n",
       "  408,\n",
       "  8,\n",
       "  3076,\n",
       "  25,\n",
       "  2555,\n",
       "  1,\n",
       "  335,\n",
       "  317,\n",
       "  118,\n",
       "  603,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  111,\n",
       "  40,\n",
       "  20,\n",
       "  13,\n",
       "  1,\n",
       "  3,\n",
       "  1200,\n",
       "  179,\n",
       "  4,\n",
       "  283,\n",
       "  3185,\n",
       "  2,\n",
       "  24,\n",
       "  876,\n",
       "  29,\n",
       "  135,\n",
       "  102,\n",
       "  22,\n",
       "  1,\n",
       "  3,\n",
       "  382,\n",
       "  607,\n",
       "  1440,\n",
       "  10,\n",
       "  2,\n",
       "  715,\n",
       "  83,\n",
       "  1,\n",
       "  2218,\n",
       "  25,\n",
       "  666,\n",
       "  11,\n",
       "  18,\n",
       "  12,\n",
       "  26,\n",
       "  63,\n",
       "  1817,\n",
       "  4,\n",
       "  12,\n",
       "  11,\n",
       "  383,\n",
       "  1033,\n",
       "  1312,\n",
       "  562]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep model feature: Word embeddings of wine descriptions\n",
    "train_embed = tokenize.texts_to_sequences(f_description)\n",
    "train_embed[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve got integerized description vectors, we need to make sure they’re all the same length to feed them into our model. Keras has a handy method for that too. We’ll use pad_sequences to add zeros to each description vector so that they’re all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           7, 1695,  397,  408,    8, 3076,   25, 2555,    1,  335,  317,\n",
       "         118,  603,   63,   10,   39,  111,   40,   20,   13,    1,    3,\n",
       "        1200,  179,    4,  283, 3185,    2,   24,  876,   29,  135,  102,\n",
       "          22,    1,    3,  382,  607, 1440,   10,    2,  715,   83,    1,\n",
       "        2218,   25,  666,   11,   18,   12,   26,   63, 1817,    4,   12,\n",
       "          11,  383, 1033, 1312,  562],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          27,   19,    4,  560,   50,    1,  142,   28, 1630,    1, 2382,\n",
       "          29,    3,    4,  289,   65,    1,   55,    7,    6,   52,  419,\n",
       "         187,    1, 2800,   15,    2,   24,    5,   37,    9,    4,  670,\n",
       "          30,   66,    1,  594,  247,    3,  239,   18,    6, 1299,   14,\n",
       "        7670,   83,   32,   80, 3409]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 170\n",
    "train_embed = keras.preprocessing.sequence.pad_sequences(train_embed, maxlen=max_seq_length)\n",
    "train_embed[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our descriptions converted to vectors that are all the same length, we’re ready to create our embedding layer and feed it into a deep model.\n",
    "\n",
    "**Word Embedding**: First, we’ll define the shape of our inputs for Keras-Function-API-Model. Then we’ll feed it into the Embedding layer.\n",
    "\n",
    "Here I’m using an Embedding layer with 8 dimensions.  Dimensions of embedding space correspond to the following axes of variation. \n",
    "  1. Dimension 1: How old the wine?\n",
    "  2. Dimension 2: How acidic flavour?\n",
    "  3. Dimension 3: How mature is the intended reviewer?\n",
    "  4. etc.\n",
    "\n",
    "\n",
    "The output of the Embedding layer will be a three dimensional vector with shape: [batch size, sequence length (170 in this example), embedding dimension (8 in this example)]. In order to connect our Embedding layer to the Dense, fully connected output layer we need to flatten it first:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_inputs = keras.layers.Input(shape=(max_seq_length,))\n",
    "embedding = keras.layers.Embedding(vocab_size, 8,   input_length=max_seq_length)(deep_inputs)\n",
    "embedding = keras.layers.Flatten()(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the embedding layer is flattened it’s ready to feed into the model and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 170, 8)            96000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1360)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1361      \n",
      "=================================================================\n",
      "Total params: 97,361\n",
      "Trainable params: 97,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_out = keras.layers.Dense(1, activation='linear')(embedding)\n",
    "deep_model = keras.Model(inputs=deep_inputs, outputs=embed_out)\n",
    "print(deep_model.summary())\n",
    "deep_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
